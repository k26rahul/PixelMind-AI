{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOr6Lu9/x6XpPaFQ214Vu2Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PixelMind AI"],"metadata":{"id":"5fe3XtP8ZhIb"}},{"cell_type":"code","source":["#used to access files\n","import os\n","\n","#used to generate random numbers or floats\n","import random\n","\n","# used for array manipulation\n","import numpy as np\n","\n","# used to serialize the data from folder for training/testing/validation\n","from glob import glob\n"," \n"," # used for final image comparison\n","from PIL import Image, ImageOps\n","\n","import cv2"],"metadata":{"id":"xptvrdFhlZ1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"WrVYhRAllnDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creating a TensorFlow Dataset"],"metadata":{"id":"QcClOmFjmYCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IMAGE_SIZE = 4288, 2848\n","BATCH_SIZE = 8\n","MAX_TRAIN_IMAGES = 1400\n","MAX_VAL_IMAGES = 1490\n","\n","# Split the dataset\n","train_low_light_images = sorted(glob(\"./drive/MyDrive/FinalDataset_Merged/train_indoor/Raw/*\"))[:MAX_TRAIN_IMAGES]\n","val_low_light_images = sorted(glob(\"./drive/MyDrive/FinalDataset_Merged/train_indoor/Raw/*\"))[MAX_TRAIN_IMAGES:MAX_VAL_IMAGES]\n","test_low_light_images = sorted(glob(\"./drive/MyDrive/FinalDataset_Merged/train_indoor/Raw/*\"))[MAX_VAL_IMAGES:]\n","\n","def load_data(image_path):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_png(image, channels=3) # Decode the image\n","    #image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","    image = image / 2848 #normalize image\n","    return image\n","\n","\n","def data_generator(low_light_images):\n","    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n","    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    return dataset\n","\n","train_dataset = data_generator(train_low_light_images)\n","val_dataset = data_generator(val_low_light_images)\n","\n","print(\"Train Dataset:\", train_dataset)\n","print(\"Validation Dataset:\", val_dataset)\n","\n"],"metadata":{"id":"-w8zmSohlqqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\"\"\"## The Zero-DCE Framework\n","\n","The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n","(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n","channels by applying the curves iteratively to obtain the final enhanced image.\n","\n","### Understanding light-enhancement curves\n","\n","A ligh-enhancement curve is a kind of curve that can map a low-light image\n","to its enhanced version automatically,\n","where the self-adaptive curve parameters are solely dependent on the input image.\n","When designing such a curve, three objectives should be taken into account:\n","\n","- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n","avoid information loss induced by overflow truncation.\n","- It should be monotonous, to preserve the contrast between neighboring pixels.\n","- The shape of this curve should be as simple as possible,\n","and the curve should be differentiable to allow backpropagation.\n","\n","The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n","illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n","the risk of over-saturation.\n","\n","![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n","\n","### DCE-Net\n","\n","The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n","image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n","image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n","higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n","concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n","followed by the ReLU activation function. The last convolutional layer is followed by the\n","Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n","iteration requires three curve parameter maps for the three channels.\n","\n","![](https://i.imgur.com/HtIg34W.png)\n","\"\"\"\n","\n"],"metadata":{"id":"MBp83rM-mQoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def build_dce_net():\n","    input_img = keras.Input(shape=[None, None, 3])\n","    conv1 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(input_img)\n","    conv2 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(conv1)\n","    conv3 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(conv2)\n","    conv4 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(conv3)\n","    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n","    conv5 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(int_con1)\n","    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n","    conv6 = layers.Conv2D(\n","        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n","    )(int_con2)\n","    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n","    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n","        int_con3\n","    )\n","    return keras.Model(inputs=input_img, outputs=x_r)\n","\n"],"metadata":{"id":"x98_rFipmNt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"## Loss functions\n","\n","To enable zero-reference learning in DCE-Net, we use a set of differentiable\n","zero-reference losses that allow us to evaluate the quality of enhanced images.\n","\n","### Color constancy loss\n","\n","The *color constancy loss* is used to correct the potential color deviations in the\n","enhanced image.\n","\"\"\"\n","\n","def color_constancy_loss(x):\n","    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n","    mr, mg, mb = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n","    d_rg = tf.square(mr - mg)\n","    d_rb = tf.square(mr - mb)\n","    d_gb = tf.square(mb - mg)\n","    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n","\n"],"metadata":{"id":"o5z6fOZ7mL3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"### Exposure loss\n","\n","To restrain under-/over-exposed regions, we use the *exposure control loss*.\n","It measures the distance between the average intensity value of a local region\n","and a preset well-exposedness level (set to `0.6`).\n","\"\"\"\n","\n","def exposure_loss(x, mean_val=0.6):\n","    x = tf.reduce_mean(x, axis=3, keepdims=True)\n","    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n","    return tf.reduce_mean(tf.square(mean - mean_val))\n","\n"],"metadata":{"id":"Jsqw2VyTmKh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"### Illumination smoothness loss\n","\n","To preserve the monotonicity relations between neighboring pixels, the\n","*illumination smoothness loss* is added to each curve parameter map.\n","\"\"\"\n","\n","def illumination_smoothness_loss(x):\n","    batch_size = tf.shape(x)[0]\n","    h_x = tf.shape(x)[1]\n","    w_x = tf.shape(x)[2]\n","    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n","    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n","    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n","    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n","    batch_size = tf.cast(batch_size, dtype=tf.float32)\n","    count_h = tf.cast(count_h, dtype=tf.float32)\n","    count_w = tf.cast(count_w, dtype=tf.float32)\n","    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n"],"metadata":{"id":"XNJZ5HIumIp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"### Spatial consistency loss\n","\n","The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n","preserving the contrast between neighboring regions across the input image and its enhanced version.\n","\"\"\"\n","\n","class SpatialConsistencyLoss(keras.losses.Loss):\n","    def __init__(self, **kwargs):\n","        super(SpatialConsistencyLoss, self).__init__(reduction=\"none\")\n","\n","        self.left_kernel = tf.constant(\n","            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n","        )\n","        self.right_kernel = tf.constant(\n","            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n","        )\n","        self.up_kernel = tf.constant(\n","            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n","        )\n","        self.down_kernel = tf.constant(\n","            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n","        )\n","\n","    def call(self, y_true, y_pred):\n","\n","        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n","        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n","        original_pool = tf.nn.avg_pool2d(\n","            original_mean, ksize=4, strides=4, padding=\"VALID\"\n","        )\n","        enhanced_pool = tf.nn.avg_pool2d(\n","            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n","        )\n","\n","        d_original_left = tf.nn.conv2d(\n","            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_original_right = tf.nn.conv2d(\n","            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_original_up = tf.nn.conv2d(\n","            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_original_down = tf.nn.conv2d(\n","            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","\n","        d_enhanced_left = tf.nn.conv2d(\n","            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_enhanced_right = tf.nn.conv2d(\n","            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_enhanced_up = tf.nn.conv2d(\n","            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","        d_enhanced_down = tf.nn.conv2d(\n","            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n","        )\n","\n","        d_left = tf.square(d_original_left - d_enhanced_left)\n","        d_right = tf.square(d_original_right - d_enhanced_right)\n","        d_up = tf.square(d_original_up - d_enhanced_up)\n","        d_down = tf.square(d_original_down - d_enhanced_down)\n","        return d_left + d_right + d_up + d_down\n","\n"],"metadata":{"id":"Bnb13LbImG5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"### Deep curve estimation model\n","\n","We implement the Zero-DCE framework as a Keras subclassed model.\n","\"\"\"\n","\n","class ZeroDCE(keras.Model):\n","    def __init__(self, **kwargs):\n","        super(ZeroDCE, self).__init__(**kwargs)\n","        self.dce_model = build_dce_net()\n","\n","    def compile(self, learning_rate, **kwargs):\n","        super(ZeroDCE, self).compile(**kwargs)\n","        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n","\n","    def get_enhanced_image(self, data, output):\n","        r1 = output[:, :, :, :3]\n","        r2 = output[:, :, :, 3:6]\n","        r3 = output[:, :, :, 6:9]\n","        r4 = output[:, :, :, 9:12]\n","        r5 = output[:, :, :, 12:15]\n","        r6 = output[:, :, :, 15:18]\n","        r7 = output[:, :, :, 18:21]\n","        r8 = output[:, :, :, 21:24]\n","        x = data + r1 * (tf.square(data) - data)\n","        x = x + r2 * (tf.square(x) - x)\n","        x = x + r3 * (tf.square(x) - x)\n","        enhanced_image = x + r4 * (tf.square(x) - x)\n","        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n","        x = x + r6 * (tf.square(x) - x)\n","        x = x + r7 * (tf.square(x) - x)\n","        enhanced_image = x + r8 * (tf.square(x) - x)\n","        return enhanced_image\n","\n","    def call(self, data):\n","        dce_net_output = self.dce_model(data)\n","        return self.get_enhanced_image(data, dce_net_output)\n","\n","    def compute_losses(self, data, output):\n","        enhanced_image = self.get_enhanced_image(data, output)\n","        loss_illumination = 200 * illumination_smoothness_loss(output)\n","        loss_spatial_constancy = tf.reduce_mean(\n","            self.spatial_constancy_loss(enhanced_image, data)\n","        )\n","        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n","        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n","        total_loss = (\n","            loss_illumination\n","            + loss_spatial_constancy\n","            + loss_color_constancy\n","            + loss_exposure\n","        )\n","        return {\n","            \"total_loss\": total_loss,\n","            \"illumination_smoothness_loss\": loss_illumination,\n","            \"spatial_constancy_loss\": loss_spatial_constancy,\n","            \"color_constancy_loss\": loss_color_constancy,\n","            \"exposure_loss\": loss_exposure,\n","        }\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            output = self.dce_model(data)\n","            losses = self.compute_losses(data, output)\n","        gradients = tape.gradient(\n","            losses[\"total_loss\"], self.dce_model.trainable_weights\n","        )\n","        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n","        return losses\n","\n","    def test_step(self, data):\n","        output = self.dce_model(data)\n","        return self.compute_losses(data, output)\n","\n","    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n","        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n","        self.dce_model.save_weights(\n","            filepath, overwrite=overwrite, save_format=save_format, options=options\n","        )\n","\n","    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n","        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n","        self.dce_model.load_weights(\n","            filepath=filepath,\n","            by_name=by_name,\n","            skip_mismatch=skip_mismatch,\n","            options=options,\n","        )\n","\n"],"metadata":{"id":"43UKgrIpmEJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"## Training\"\"\"\n","\n","zero_dce_model = ZeroDCE()\n","zero_dce_model.compile(learning_rate=0.4) \n","history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=200)\n","\n","\n","def plot_result(item):\n","    plt.plot(history.history[item], label=item)\n","    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(item)\n","    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n"],"metadata":{"id":"PQL_dnGvmBxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plot_result(\"total_loss\")\n","plot_result(\"illumination_smoothness_loss\")\n","plot_result(\"spatial_constancy_loss\")\n","plot_result(\"color_constancy_loss\")\n","plot_result(\"exposure_loss\")\n","\n"],"metadata":{"id":"KTUN7Vm_mAdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"## Inference\"\"\"\n","\n","def plot_results(images, titles, figure_size=(12, 12)):\n","    fig = plt.figure(figsize=figure_size)\n","    for i in range(len(images)):\n","        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n","        _ = plt.imshow(images[i])\n","        plt.axis(\"off\")\n","    plt.show()\n","\n","\n","def infer(original_image):\n","    image = keras.preprocessing.image.img_to_array(original_image)\n","    image = image.astype(\"float32\") / 255.0\n","    image = np.expand_dims(image, axis=0)\n","    output_image = zero_dce_model(image)\n","    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n","    output_image = Image.fromarray(output_image.numpy())\n","    return output_image\n","\n"],"metadata":{"id":"pJFxgHKZl-8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"### Inference on test images and visualize results using W&B Tables\n","\n","\"\"\"\n","\n","wandb.init(project=\"low_light_zero_DCE\", job_type=\"predictions\")\n","\n","table = wandb.Table(columns=[\"Original\", \"PIL Autocontrast\", \"Enhanced\"])\n","for val_image_file in test_low_light_images:\n","    original_image = Image.open(val_image_file)\n","    enhanced_image = infer(original_image)\n","    table.add_data(\n","        wandb.Image(np.array(original_image)),\n","        wandb.Image(np.array(ImageOps.autocontrast(original_image))),\n","        wandb.Image(np.array(enhanced_image))\n","    )\n","\n","wandb.log({\"Inference Table\": table})\n","\n","wandb.finish()\n"],"metadata":{"id":"UOFVkATjl9hG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def get_size_format(b, factor=1024, suffix=\"B\"):"],"metadata":{"id":"tx8zSk9Tlu8E"},"execution_count":null,"outputs":[]}]}